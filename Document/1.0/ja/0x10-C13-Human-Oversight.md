# C13 人間による監視、説明責任、ガバナンス (Human Oversight, Accountability & Governance)

## 管理目標

この章では、AI システムでの人間の監視と明確な説明責任の連鎖を維持し、AI ライフサイクル全体にわたって説明可能性、透明性、倫理的管理責任を確保するための要件を提供します。

---

## C13.1 キルスイッチとオーバーライドメカニズム (Kill-Switch & Override Mechanisms)

AI システムの安全でない動作が観察された場合、シャットダウンまたはロールバックパスを提供します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.1.1** | **検証:** 手動キルスイッチメカニズムは、AI モデルの推論と出力を即座に停止するために、存在している。 | 1 | D/V |
| **13.1.2** | **検証:** オーバーライドコントロールは認可された担当者のみがアクセスできる。 | 1 | D |
| **13.1.3** | **検証:** ロールバック手順は以前のモデルバージョンまたはセーフモード操作に戻ることができる。 | 3 | D/V |
| **13.1.4** | **検証:** オーバーライドメカニズムは定期的にテストされている。 | 3 | V |

---

## C13.2 ヒューマンインザループの意思決定チェックポイント (Human-in-the-Loop Decision Checkpoints)

ステークスが事前定義されたリスク閾値を超える場合は人間の承認を必要とします。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.2.1** | **検証:** リスクの高い AI による決定は実行前に明示的な人間の承認を必要としている。 | 1 | D/V |
| **13.2.2** | **検証:** リスク閾値は明確に定義されており、人間のレビューワークフローを自動的にトリガーしている。 | 1 | D |
| **13.2.3** | **検証:** 時間的制約のある決定は、人間の承認が必要な時間枠内に得られない場合に、フォールバック手順を用意している。 | 2 | D |
| **13.2.4** | **検証:** エスカレーション手順は、該当する場合、さまざまな意思決定タイプまたはリスクカテゴリに対して明確な権限レベルを定義している。 | 3 | D/V |

---

## C13.3 責任の連鎖と監査可能性 (Chain of Responsibility & Auditability)

オペレータのアクションとモデルの決定をログ記録します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.3.1** | **検証:** すべての AI システムの決定と人間の介入は、タイムスタンプ、ユーザーアイデンティティ、決定の根拠とともにログ記録されている。 | 1 | D/V |
| **13.3.2** | **検証:** 監査ログは改竄できず、完全性検証メカニズムを含んでいる。 | 2 | D |

---

## C13.4 説明可能な AI 技法 (Explainable-AI Techniques)

特徴量の重要度、反事実、局所的説明を提示します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.4.1** | **検証:** AI システムは人間が読み取り可能な形式でその決定について基本的な説明を提供している。 | 1 | D/V |
| **13.4.2** | **検証:** 説明の品質は人間による評価研究とメトリクスを通じて検証されている。 | 2 | V |
| **13.4.3** | **検証:** 特徴量の重要度スコアまたは帰属手法 (SHAP, LIME など) は重要な決定に利用できる。 | 3 | D/V |
| **13.4.4** | **検証:** 反事実的説明は、ユースケースとドメインに当てはまる場合、入力がどのように改変されると出力が変わるかを示している。 | 3 | V |

---

## C13.5 モデルカードと使用状況開示 (Model Cards & Usage Disclosures)

意図した使用、パフォーマンスメトリクス、倫理的考慮事項についてモデルカードを維持します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.5.1** | **検証:** モデルカードは、意図したユースケース、制限、既知の障害モードを記載している。 | 1 | D |
| **13.5.2** | **検証:** さまざまな適用可能なユースケースにわたるパフォーマンスメトリクスは公開されている。 | 1 | D/V |
| **13.5.3** | **検証:** 倫理的考慮事項、バイアス評価、公平性評価、トレーニングデータの特性、既知のとレーニンデータの制限は文書化され、定期的に更新されている。 | 2 | D |
| **13.5.4** | **検証:** モデルカードはバージョン管理され、変更追跡とともにモデルのライフサイクル全体にわたって維持されている。 | 2 | D/V |

---

## C13.6 不確実性の定量化 (Uncertainty Quantification)

信頼スコアまたはエントロピー測定値をレスポンスに伝播します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.6.1** | **検証:** AI システムは出力とともに信頼スコアまたは不確実性測定値を提供している。 | 1 | D |
| **13.6.2** | **検証:** 不確実性閾値は追加の人間のレビューまたは代替の意思決定経路をトリガーしている。 | 2 | D/V |
| **13.6.3** | **検証:** 不確実性の定量化手法はグラウンドトゥルースデータに対して調整および検証されている。 | 2 | V |
| **13.6.4** | **検証:** 不確実性の伝播は複数ステップの AI ワークフローを通じて維持されている。 | 3 | D/V |

---

## C13.7 ユーザー向け透明性レポート (User-Facing Transparency Reports)

インシデント、ドリフト、データ使用状況について定期的に開示します。

| # | 説明 | レベル | ロール |
|:--------:|---------------------------------------------------------------------------------------------------------------------|:---:|:---:|
| **13.7.1** | **検証:** データ使用ポリシーとユーザー同意管理の実践は利害関係者に明確に伝達されている。 | 1 | D/V |
| **13.7.2** | **検証:** AI 影響評価は実施されており、結果は報告書に含められている。 | 2 | D/V |
| **13.7.3** | **検証:** 定期的に公開される透明性レポートは AI インシデントおよび運用メトリクスを適切な詳細度で開示している。 | 2 | D/V |

### 参考情報

* [EU Artificial Intelligence Act — Regulation (EU) 2024/1689 (Official Journal, 12 July 2024)](https://eur-lex.europa.eu/eli/reg/2024/1689/oj)
* [ISO/IEC 23894:2023 — Artificial Intelligence — Guidance on Risk Management](https://www.iso.org/standard/77304.html)
* [ISO/IEC 42001:2023 — AI Management Systems Requirements](https://www.iso.org/standard/81230.html)
* [NIST AI Risk Management Framework 1.0](https://nvlpubs.nist.gov/nistpubs/ai/nist.ai.100-1.pdf)
* [NIST SP 800-53 Revision 5 — Security and Privacy Controls](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf)
* [A Unified Approach to Interpreting Model Predictions (SHAP, ICML 2017)](https://arxiv.org/abs/1705.07874)
* [Model Cards for Model Reporting (Mitchell et al., 2018)](https://arxiv.org/abs/1810.03993)
* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (Gal & Ghahramani, 2016)](https://arxiv.org/abs/1506.02142)
* [ISO/IEC 24029-2:2023 — Robustness of Neural Networks — Methodology for Formal Methods](https://www.iso.org/standard/79804.html)
* [IEEE 7001-2021 — Transparency of Autonomous Systems](https://standards.ieee.org/ieee/7001/6929/)
* [Human Oversight under Article 14 of the EU AI Act (Fink, 2025)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5147196)
